{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I started with gathering data from different sources i. e twitter-archive-enhanced.csv ,image_predictions link and tweet_json.txt . The data was then loaded into three dataframes i.e  twitter_df,img_df and json_df . Visual assessment was done by inspecting the dataframes and a number of quality and tidiness issues picked such as  invalid dognames i.e ‘a’,’an’,’the’ and multiple columns for dog stage name. Programmatic assessment was the done by executing a number of pandas functions. Executing .info() method showed that some columns had wrong datatypes. Describe() method showed the presence of outliers in the rating_numerator and rating_denominator which can potentially stretch the means of the respective variables. Isnull() was used to check for the presence of null values in the dataframes and some nulls were picked in the img_df dataframe . </p>\n",
    "<p>\n",
    "The invalid dog names were partially corrected by using the text column of twitter_df dataframe . I first extracted the first sentence from text column and checked which sentence had “This is a/an/the” format using a regular expression. The words after a/an/the were then used to form the word names which were used to correct the erroneous names in twitter_df . The multiple columns for dog stage names were reduced into one column called “Dog_stage” and populated appropriately.  \n",
    "</p>\n",
    "<p>\n",
    "Nulls in img_df were dropped because the null extracted was persistent across all the columns of the dataframe. The columns with wrong datatypes were converted to the appropriate datatypes by using relevant functions such as pd.to_datetime and .astype.\n",
    "\n",
    "</p>\n",
    "<p>\n",
    "    The erroneous ratings were corrected. For the denominator rating, all values which were not 10 were dropped since it was earlier explained that 10 was the correct value. For the presence of relatively high values in the numerator such as (above 1700),i dropped all values above the 99 percentile value and the maximum value dropped to less than 15 which is more reflective of the rating.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
